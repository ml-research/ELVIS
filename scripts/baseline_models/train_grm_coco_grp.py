# Created by MacBook Pro at 27.11.25

import random
import time

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import wandb
import argparse
from rtpt import RTPT
from typing import List
import json
from pathlib import Path
from collections import defaultdict
from torch.utils.data import Dataset
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score
import numpy as np
from PIL import Image, ImageDraw, ImageFont



from scripts.baseline_models.grm import ContextContourScorer, GroupingTransformer, debug_tiny_mlp, PairOnlyTransformer
from scripts import config
from scripts.baseline_models.bm_utils import load_images, load_jsons, load_patterns, get_obj_imgs, preprocess_rgb_image_to_patch_set_batch, process_object_pairs


def balance_data(data_pairs):
    """Balance positive and negative pairs"""
    pos_pairs = [p for p in data_pairs if p[3] == 1]
    neg_pairs = [p for p in data_pairs if p[3] == 0]
    min_size = min(len(pos_pairs), len(neg_pairs))

    if min_size == 0:
        print(f"Warning: No positive or negative pairs found!")
        return data_pairs

    balanced = random.sample(pos_pairs, min_size) + random.sample(neg_pairs, min_size)
    print(f"Balanced data: {min_size} pos + {min_size} neg = {len(balanced)} total")
    return balanced


def load_grm_grp_data(task_num, img_num, principle_path, num_patches, points_per_patch):
    pattern_folders = load_patterns(principle_path, 0, "end")
    # random.seed(42)
    # random.shuffle(pattern_folders)
    pattern_folders = pattern_folders[:task_num]
    train_data = []
    test_data = []
    task_names = []
    for pattern_folder in pattern_folders:
        train_imgs = load_images(pattern_folder / "positive", img_num) + load_images(pattern_folder / "negative", img_num)
        train_jsons = load_jsons(pattern_folder / "positive", img_num) + load_jsons(pattern_folder / "negative", img_num)

        test_imgs = load_images((principle_path / "test" / pattern_folder.name) / "positive", img_num) + load_images((principle_path / "test" / pattern_folder.name) / "negative",
                                                                                                                     img_num)
        test_jsons = load_jsons((principle_path / "test" / pattern_folder.name) / "positive", img_num) + load_jsons((principle_path / "test" / pattern_folder.name) / "negative",
                                                                                                                    img_num)

        # extrac object images and preprocess
        pattern_data_train = []
        pattern_data_test = []

        for img, json_data in zip(test_imgs, test_jsons):
            obj_imgs = get_obj_imgs(img, json_data)
            y_true = [data["group_id"] for data in json_data["img_data"]]
            patch_sets, positions, sizes = preprocess_rgb_image_to_patch_set_batch(obj_imgs, num_patches=num_patches,
                                                                                   points_per_patch=points_per_patch)
            pattern_data_test.append((patch_sets, positions, sizes, y_true))
        for img, json_data in zip(train_imgs, train_jsons):
            obj_imgs = get_obj_imgs(img, json_data)
            y_true = [data["group_id"] for data in json_data["img_data"]]
            patch_sets, positions, sizes = preprocess_rgb_image_to_patch_set_batch(obj_imgs, num_patches=num_patches,
                                                                                   points_per_patch=points_per_patch)
            pattern_data_train.append((patch_sets, positions, sizes, y_true))

        train_pairs = []
        for patch_sets, positions, sizes, y_true in pattern_data_train:
            pair_data = process_object_pairs(patch_sets, y_true, device, points_per_patch, num_patches)
            train_pairs.extend(pair_data)

        test_pairs = []
        for patch_sets, positions, sizes, y_true in pattern_data_test:
            pair_data = process_object_pairs(patch_sets, y_true, device, points_per_patch, num_patches)
            test_pairs.extend(pair_data)

        train_data.extend(train_pairs)
        test_data.extend(test_pairs)
        task_names.append(pattern_folder.name)

    train_data = balance_data(train_data)
    test_data = balance_data(test_data)
    return train_data, test_data, task_names



def load_coco_prox_data(
    args,
    data_split: str = "val",
    normalize: bool = True,
    device: str = "cpu",
):
    """
    Load the small COCO proximity dataset generated by build_proximity_dataset().

    Returns:
        samples: list of dicts, one per image, each containing:
            {
                "image_id": int,
                "file_name": str,
                "image_path": Path,
                "width": int,
                "height": int,
                "centers": Tensor [N, 2],   # (cx, cy), normalized if normalize=True
                "bboxes": Tensor [N, 4],    # (x, y, w, h) in original pixels
                "group_ids": Tensor [N],    # int64 group ids
                "ann_ids": Tensor [N],      # original COCO annotation ids
            }
    """
    coco_root = config.get_coco_path(args.remote)
    ann_path = coco_root / "selected" / "annotations" / f"proximity_{data_split}2017.json"
    img_dir = coco_root / "selected" / f"{data_split}2017"

    if not ann_path.exists():
        raise FileNotFoundError(f"Proximity annotation file not found: {ann_path}")
    if not img_dir.exists():
        raise FileNotFoundError(f"Image directory not found: {img_dir}")

    with open(ann_path, "r") as f:
        coco = json.load(f)

    images = coco["images"]
    annotations = coco["annotations"]

    # Build mapping: image_id -> image info
    img_by_id = {img["id"]: img for img in images}

    # Build mapping: image_id -> annotations
    img_to_anns = defaultdict(list)
    for ann in annotations:
        img_to_anns[ann["image_id"]].append(ann)

    samples = []

    for img_id, img_info in img_by_id.items():
        anns = img_to_anns.get(img_id, [])
        if len(anns) == 0:
            # Should not happen, but skip just in case
            continue

        w, h = img_info["width"], img_info["height"]
        file_name = img_info["file_name"]
        img_path = img_dir / file_name

        centers = []
        bboxes = []
        group_ids = []
        ann_ids = []

        for ann in anns:
            x, y, bw, bh = ann["bbox"]
            cx = x + bw / 2.0
            cy = y + bh / 2.0

            if normalize:
                cx_norm = cx / w
                cy_norm = cy / h
                centers.append([cx_norm, cy_norm])
            else:
                centers.append([cx, cy])

            bboxes.append([x, y, bw, bh])
            group_ids.append(ann["group_id"])
            ann_ids.append(ann["id"])

        centers = torch.tensor(centers, dtype=torch.float32, device=device)
        bboxes = torch.tensor(bboxes, dtype=torch.float32, device=device)
        group_ids = torch.tensor(group_ids, dtype=torch.long, device=device)
        ann_ids = torch.tensor(ann_ids, dtype=torch.long, device=device)

        samples.append(
            {
                "image_id": img_id,
                "file_name": file_name,
                "image_path": img_path,
                "width": w,
                "height": h,
                "centers": centers,
                "bboxes": bboxes,
                "group_ids": group_ids,
                "ann_ids": ann_ids,
            }
        )

    print(f"[load_coco_prox_data] Loaded {len(samples)} images from {ann_path}")
    return samples

class PairDataset(Dataset):
    def __init__(self, contours_i: torch.Tensor, contours_j: torch.Tensor, labels: torch.Tensor):
        """
        contours_i: [M, P, L, D]
        contours_j: [M, P, L, D]
        labels    : [M]
        """
        assert contours_i.shape == contours_j.shape
        assert contours_i.shape[0] == labels.shape[0]
        self.contours_i = contours_i
        self.contours_j = contours_j
        self.labels = labels

    def __len__(self):
        return self.labels.shape[0]

    def __getitem__(self, idx):
        # each item: (P, L, D), (P, L, D), scalar
        return self.contours_i[idx], self.contours_j[idx], self.labels[idx]


def _build_object_features(sample, input_type: str):
    """
    From one sample dict returned by load_coco_prox_data(), build
    per-object feature vectors according to input_type.

    sample:
      "centers": [N, 2] normalized (cx, cy)
      "bboxes" : [N, 4] (x, y, w, h) in pixels
      "width", "height"
    """
    centers = sample["centers"]   # [N, 2]
    bboxes = sample["bboxes"]     # [N, 4]
    w_img = float(sample["width"])
    h_img = float(sample["height"])

    feats = []
    for i in range(centers.shape[0]):
        cx, cy = centers[i].tolist()
        x, y, bw, bh = bboxes[i].tolist()

        vec = []

        # positional part
        if "pos" in input_type:
            vec.extend([cx, cy])

        # fake color (zeros for now; can be replaced by real color later)
        if "color" in input_type:
            vec.extend([0.0, 0.0, 0.0])

        # size part from bbox (normalized width/height)
        if "size" in input_type:
            bw_norm = bw / w_img if w_img > 0 else 0.0
            bh_norm = bh / h_img if h_img > 0 else 0.0
            vec.extend([bw_norm, bh_norm])

        feats.append(vec)

    feats = torch.tensor(feats, dtype=torch.float32)  # [N, D]
    return feats



def _build_pair_dataset_from_samples(
    samples,
    input_type: str,
    num_patches: int,
    points_per_patch: int,
    data_num: int = 1000,
    seed: int = 0,
):
    random.seed(seed)

    pos_pairs_i, pos_pairs_j = [], []
    neg_pairs_i, neg_pairs_j = [], []
    pos_labels, neg_labels = [], []

    for sample in samples:
        group_ids = sample["group_ids"]
        if group_ids.numel() < 2:
            continue

        obj_feats = _build_object_features(sample, input_type)  # [N, D]
        N, D = obj_feats.shape

        # group -> indices
        group_to_idx = {}
        for idx, g in enumerate(group_ids.tolist()):
            group_to_idx.setdefault(g, []).append(idx)

        # positive pairs: within each group
        for g, idxs in group_to_idx.items():
            if len(idxs) < 2:
                continue
            for i in range(len(idxs)):
                for j in range(i + 1, len(idxs)):
                    fi = obj_feats[idxs[i]]
                    fj = obj_feats[idxs[j]]
                    base_i = fi.view(1, 1, D)
                    base_j = fj.view(1, 1, D)
                    contour_i = base_i.expand(num_patches, points_per_patch, D)
                    contour_j = base_j.expand(num_patches, points_per_patch, D)

                    pos_pairs_i.append(contour_i)
                    pos_pairs_j.append(contour_j)
                    pos_labels.append(1.0)

        # negative pairs: across groups
        unique_groups = list(group_to_idx.keys())
        if len(unique_groups) >= 2:
            for i_g in range(len(unique_groups)):
                for j_g in range(i_g + 1, len(unique_groups)):
                    gi = unique_groups[i_g]
                    gj = unique_groups[j_g]
                    for idx_i in group_to_idx[gi]:
                        for idx_j in group_to_idx[gj]:
                            fi = obj_feats[idx_i]
                            fj = obj_feats[idx_j]
                            base_i = fi.view(1, 1, D)
                            base_j = fj.view(1, 1, D)
                            contour_i = base_i.expand(num_patches, points_per_patch, D)
                            contour_j = base_j.expand(num_patches, points_per_patch, D)

                            neg_pairs_i.append(contour_i)
                            neg_pairs_j.append(contour_j)
                            neg_labels.append(0.0)

    # Now balance
    print(f"[pair stats raw] pos={len(pos_pairs_i)}, neg={len(neg_pairs_i)}")
    if len(pos_pairs_i) == 0 or len(neg_pairs_i) == 0:
        raise RuntimeError("No positive or negative pairs collected.")

    # choose min count, and optionally cap by data_num/2
    max_per_class = min(len(pos_pairs_i), len(neg_pairs_i))
    if data_num is not None and data_num > 0:
        max_per_class = min(max_per_class, data_num // 2)

    # subsample
    pos_idx = list(range(len(pos_pairs_i)))
    neg_idx = list(range(len(neg_pairs_i)))
    random.shuffle(pos_idx)
    random.shuffle(neg_idx)
    pos_idx = pos_idx[:max_per_class]
    neg_idx = neg_idx[:max_per_class]

    contours_i_list = [pos_pairs_i[i] for i in pos_idx] + [neg_pairs_i[i] for i in neg_idx]
    contours_j_list = [pos_pairs_j[i] for i in pos_idx] + [neg_pairs_j[i] for i in neg_idx]
    labels_list     = [pos_labels[i]   for i in pos_idx] + [neg_labels[i]   for i in neg_idx]

    # shuffle all
    idxs = list(range(len(contours_i_list)))
    random.shuffle(idxs)

    contours_i = torch.stack([contours_i_list[i] for i in idxs], dim=0)  # [M, P, L, D]
    contours_j = torch.stack([contours_j_list[i] for i in idxs], dim=0)
    labels     = torch.tensor([labels_list[i] for i in idxs], dtype=torch.float32)

    print(
        f"[train_model] Built balanced pair dataset: "
        f"{contours_i.shape[0]} pairs, pos={int(labels.sum().item())}, "
        f"neg={contours_i.shape[0] - int(labels.sum().item())}"
    )

    return PairDataset(contours_i, contours_j, labels)



def visualize_predictions(model, test_samples, input_type, num_patches, points_per_patch, input_dim, device, num_vis=10):
    """
    Visualize predictions on test images by creating side-by-side comparison:
    Left: Ground truth groups (bounding boxes only, colored by group)
    Right: Model predictions (bounding boxes + lines connecting predicted pairs)
    """
    model.eval()

    # Select random samples to visualize
    vis_samples = random.sample(test_samples, min(num_vis, len(test_samples)))

    images_to_log = []

    with torch.no_grad():
        for idx, sample in enumerate(vis_samples):
            # Load original image
            img_path = sample["image_path"]
            if not img_path.exists():
                print(f"Warning: Image not found: {img_path}")
                continue

            img_orig = Image.open(img_path).convert("RGB")
            width, height = img_orig.size

            # Create two copies: one for GT, one for predictions
            img_gt = img_orig.copy()
            img_pred = img_orig.copy()

            draw_gt = ImageDraw.Draw(img_gt, "RGBA")
            draw_pred = ImageDraw.Draw(img_pred, "RGBA")

            # Get data
            group_ids = sample["group_ids"].cpu().numpy()
            bboxes = sample["bboxes"].cpu().numpy()  # [N, 4] (x, y, w, h)
            centers = sample["centers"].cpu().numpy()  # [N, 2] normalized
            N = len(group_ids)

            # Build object features
            obj_feats = _build_object_features(sample, input_type)  # [N, D]

            # Define colors for each group
            unique_groups = np.unique(group_ids)
            group_colors = {}
            for i, g in enumerate(unique_groups):
                # Generate distinct colors
                hue = (i * 137) % 360  # Golden angle for good color separation
                rgb = _hue_to_rgb(hue)
                group_colors[g] = rgb

            # Font setup
            try:
                font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 16)
                font_large = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 24)
            except:
                font = ImageFont.load_default()
                font_large = ImageFont.load_default()

            # Draw GT: bounding boxes colored by group + group connections
            for i in range(N):
                x, y, w, h = bboxes[i]
                group = group_ids[i]
                color = group_colors[group]
                draw_gt.rectangle([x, y, x + w, y + h], outline=color + (255,), width=3)
                draw_gt.text((x, y - 18), f"G{group}", fill=color + (255,), font=font)

            # Draw GT group connections (lines within same group)
            for i in range(N):
                for j in range(i + 1, N):
                    if group_ids[i] == group_ids[j]:
                        cx_i = centers[i][0] * sample["width"]
                        cy_i = centers[i][1] * sample["height"]
                        cx_j = centers[j][0] * sample["width"]
                        cy_j = centers[j][1] * sample["height"]

                        color = group_colors[group_ids[i]]
                        draw_gt.line([(cx_i, cy_i), (cx_j, cy_j)], fill=color + (128,), width=2)

            # Draw predictions: bounding boxes + predicted connections
            for i in range(N):
                x, y, w, h = bboxes[i]
                group = group_ids[i]
                color = group_colors[group]
                draw_pred.rectangle([x, y, x + w, y + h], outline=color + (255,), width=3)
                draw_pred.text((x, y - 18), f"G{group}", fill=color + (255,), font=font)

            # Predict pairwise relationships
            for i in range(N):
                for j in range(i + 1, N):
                    fi = obj_feats[i]
                    fj = obj_feats[j]

                    # Build contour tensors
                    base_i = fi.view(1, 1, -1)
                    base_j = fj.view(1, 1, -1)
                    contour_i = base_i.expand(1, num_patches, points_per_patch, -1).to(device)
                    contour_j = base_j.expand(1, num_patches, points_per_patch, -1).to(device)

                    # Empty context
                    context_list = torch.empty(
                        1, 0, num_patches, points_per_patch, input_dim,
                        device=device,
                        dtype=contour_i.dtype,
                    )

                    # Predict
                    logits = model(contour_i, contour_j, context_list)
                    prob = torch.sigmoid(logits).item()

                    # If predicted to be in same group, draw line
                    if prob >= 0.5:
                        # Denormalize centers
                        cx_i = centers[i][0] * sample["width"]
                        cy_i = centers[i][1] * sample["height"]
                        cx_j = centers[j][0] * sample["width"]
                        cy_j = centers[j][1] * sample["height"]

                        # Check if truly same group
                        is_correct = (group_ids[i] == group_ids[j])
                        line_color = (0, 255, 0, 128) if is_correct else (255, 0, 0, 128)  # Green=correct, Red=wrong

                        draw_pred.line([(cx_i, cy_i), (cx_j, cy_j)], fill=line_color, width=2)

            # Add titles
            draw_gt.text((10, 10), "Ground Truth", fill=(255, 255, 255, 255), font=font_large)
            draw_pred.text((10, 10), "Predictions", fill=(255, 255, 255, 255), font=font_large)

            # Create side-by-side image
            combined = Image.new('RGB', (width * 2, height))
            combined.paste(img_gt.convert("RGB"), (0, 0))
            combined.paste(img_pred.convert("RGB"), (width, 0))

            # Draw separator line
            draw_combined = ImageDraw.Draw(combined)
            draw_combined.line([(width, 0), (width, height)], fill=(255, 255, 255), width=3)

            images_to_log.append(wandb.Image(
                combined,
                caption=f"Image {sample['file_name']} - Left: Ground truth groups | Right: Predictions (Green=correct, Red=incorrect)"
            ))

    # Log to wandb
    if wandb.run is not None:
        wandb.log({"test_predictions": images_to_log})

    print(f"[visualize_predictions] Logged {len(images_to_log)} images to wandb")


def _hue_to_rgb(hue):
    """Convert hue (0-360) to RGB (0-255)"""
    hue = hue % 360
    c = 255
    x = int(c * (1 - abs((hue / 60) % 2 - 1)))

    if hue < 60:
        return (c, x, 0)
    elif hue < 120:
        return (x, c, 0)
    elif hue < 180:
        return (0, c, x)
    elif hue < 240:
        return (0, x, c)
    elif hue < 300:
        return (x, 0, c)
    else:
        return (c, 0, x)


def train_model(args, principle, input_type, device, log_wandb=True, n=100, epochs=10, data_num=1000):
    num_patches = args.num_patches
    points_per_patch = args.points_per_patch
    data_path = config.get_raw_patterns_path(args.remote) / f"res_{args.img_size}_pin_False" / principle
    model_name = config.get_proj_output_path(args.remote) / f"{args.backbone}_{principle}_coco_model.pt"
    model_path_best = str(model_name).replace(".pt", "_best.pt")
    model_path_latest = str(model_name).replace(".pt", "_latest.pt")

    # Input dimension
    input_dim_map = {"pos": 2, "pos_color": 5, "pos_color_size": 7, "color_size": 5}
    if input_type not in input_dim_map:
        raise ValueError(f"Unsupported input type: {input_type}")
    input_dim = input_dim_map[input_type]

    # Setup
    if args.backbone == "transformer":
        model = GroupingTransformer(num_patches=args.num_patches, points_per_patch=points_per_patch).to(device)
    elif args.backbone == "transformer_pair_only":
        model = PairOnlyTransformer(
            num_patches=args.num_patches,
            points_per_patch=points_per_patch,
            feat_dim=input_dim
        ).to(device)
    else:
        model = ContextContourScorer(input_dim=input_dim, patch_len=points_per_patch).to(device)

    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"GroupingTransformer Parameters:")
    print(f"  Total: {total_params:,}")
    print(f"  Trainable: {trainable_params:,}")

    # Breakdown by layer (optional)
    for name, param in model.named_parameters():
        print(f"  {name}: {param.numel():,} params, shape: {param.shape}")

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    # Load and split samples
    samples = load_coco_prox_data(args, data_split="val", normalize=True, device=device)

    # Split into train (80%) and test (20%)
    random.seed(getattr(args, "seed", 0))
    random.shuffle(samples)
    split_idx = int(0.8 * len(samples))
    train_samples = samples[:split_idx]
    test_samples = samples[split_idx:]

    print(f"[train_model] Split: {len(train_samples)} train, {len(test_samples)} test samples")

    # Build train dataset
    train_dataset = _build_pair_dataset_from_samples(
        train_samples,
        input_type=input_type,
        num_patches=num_patches,
        points_per_patch=points_per_patch,
        data_num=data_num,
        seed=getattr(args, "seed", 0),
    )

    # Build test dataset
    test_dataset = _build_pair_dataset_from_samples(
        test_samples,
        input_type=input_type,
        num_patches=num_patches,
        points_per_patch=points_per_patch,
        data_num=data_num // 5,  # smaller test set
        seed=getattr(args, "seed", 0) + 1,
    )

    batch_size = getattr(args, "batch_size", 128)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)

    best_test_acc = 0.0

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for contour_i, contour_j, labels in train_loader:
            contour_i = contour_i.to(device)
            contour_j = contour_j.to(device)
            labels = labels.to(device).view(-1, 1)
            B = labels.size(0)

            context_list = torch.empty(
                B, 0, num_patches, points_per_patch, input_dim,
                device=device,
                dtype=contour_i.dtype,
            )

            optimizer.zero_grad()
            logits = model(contour_i, contour_j, context_list)

            if logits.dim() == 1:
                logits = logits.view(-1, 1)

            loss = criterion(logits.view(-1), labels.view(-1))
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * labels.size(0)
            probs = torch.sigmoid(logits.view(-1))
            preds = (probs >= 0.5).long()
            train_correct += (preds == labels.view(-1).long()).sum().item()
            train_total += labels.size(0)

        avg_train_loss = train_loss / train_total if train_total > 0 else 0.0
        train_acc = train_correct / train_total if train_total > 0 else 0.0

        # Evaluation phase
        model.eval()
        test_loss = 0.0
        test_correct = 0
        test_total = 0

        # Collect all predictions and labels for metrics
        all_preds = []
        all_labels = []
        all_probs = []

        eval_start = time.time()
        with torch.no_grad():
            for contour_i, contour_j, labels in test_loader:
                contour_i = contour_i.to(device)
                contour_j = contour_j.to(device)
                labels = labels.to(device).view(-1, 1)
                B = labels.size(0)

                context_list = torch.empty(
                    B, 0, num_patches, points_per_patch, input_dim,
                    device=device,
                    dtype=contour_i.dtype,
                )

                logits = model(contour_i, contour_j, context_list)

                if logits.dim() == 1:
                    logits = logits.view(-1, 1)

                loss = criterion(logits.view(-1), labels.view(-1))
                test_loss += loss.item() * labels.size(0)

                probs = torch.sigmoid(logits.view(-1))
                preds = (probs >= 0.5).long()
                test_correct += (preds == labels.view(-1).long()).sum().item()
                test_total += labels.size(0)

                # Collect for metrics
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.view(-1).long().cpu().numpy())
                all_probs.extend(probs.cpu().numpy())

        eval_time = time.time() - eval_start

        avg_test_loss = test_loss / test_total if test_total > 0 else 0.0
        test_acc = test_correct / test_total if test_total > 0 else 0.0

        # Calculate additional metrics
        test_f1 = f1_score(all_labels, all_preds, zero_division=0)
        test_precision = precision_score(all_labels, all_preds, zero_division=0)
        test_recall = recall_score(all_labels, all_preds, zero_division=0)

        # AUC (only if we have both classes)
        try:
            test_auc = roc_auc_score(all_labels, all_probs)
        except ValueError:
            test_auc = 0.0  # Handle case where only one class is present

        print(f"[Epoch {epoch+1}/{epochs}] "
              f"train_loss={avg_train_loss:.4f}, train_acc={train_acc:.4f} | "
              f"test_loss={avg_test_loss:.4f}, test_acc={test_acc:.4f}, "
              f"test_f1={test_f1:.4f}, test_prec={test_precision:.4f}, "
              f"test_rec={test_recall:.4f}, test_auc={test_auc:.4f} "
              f"(eval time: {eval_time:.2f}s)")

        # Log to wandb if enabled
        if log_wandb and wandb.run is not None:
            wandb.log({
                "epoch": epoch + 1,
                "train_loss": avg_train_loss,
                "train_acc": train_acc,
                "test_loss": avg_test_loss,
                "test_acc": test_acc,
                "test_f1": test_f1,
                "test_precision": test_precision,
                "test_recall": test_recall,
                "test_auc": test_auc,
                "eval_time": eval_time,
            })

        # Save best model (using F1 instead of accuracy)
        if test_f1 > best_test_acc:
            best_test_acc = test_f1
            torch.save(model.state_dict(), model_path_best)
            print(f"  -> New best test_f1={best_test_acc:.4f}, saved to {model_path_best}")

    # Save latest
    torch.save(model.state_dict(), model_path_latest)
    print(f"[train_model] Training finished. Best test_f1={best_test_acc:.4f}")
    print(f"  Best model   : {model_path_best}")
    print(f"  Latest model : {model_path_latest}")

    # Visualize predictions on test set
    print("\n[train_model] Visualizing predictions on test set...")
    visualize_predictions(
        model,
        test_samples,
        input_type,
        num_patches,
        points_per_patch,
        input_dim,
        device,
        num_vis=10
    )

    return {
        "best_test_f1": best_test_acc,
        "model_path_best": model_path_best,
        "model_path_latest": model_path_latest,
    }



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--principle", type=str, default="proximity", help="Gestalt principle to train on")
    parser.add_argument("--input_type", type=str, default="pos_color_size", help="Type of input features")
    parser.add_argument("--sample_size", type=int, default=16, help="Number of points per patch")
    parser.add_argument("--img_size", type=int, default=224, help="Image size")
    parser.add_argument("--img_num", type=int, default=3, help="Number of images to load per pattern")
    parser.add_argument("--remote", action="store_true", help="Use remote data path")
    parser.add_argument("--remove_cache", action="store_true", help="Remove cache before training")
    parser.add_argument("--data_num", type=int, default=100000, help="Number of data samples to use")
    parser.add_argument("--epochs", type=int, default=20, help="Number of training epochs")
    parser.add_argument("--task_num", type=int, default=5, help="Number of training tasks")
    parser.add_argument("--num_patches", type=int, default=4, help="Number of patches per object")
    parser.add_argument("--points_per_patch", type=int, default=6, help="Number of points per patch")
    parser.add_argument("--device", type=str, default="0", help="Device to use for training")
    parser.add_argument("--backbone", type=str, default="transformer", help="Backbone model to use", choices=["mlp", "transformer",
                                                                                                              "transformer_pair_only"])
    args = parser.parse_args()
    device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() else "cpu")

    wandb.init(project="grm_grp_training", name=f"{args.principle}_{args.input_type}_size{args.sample_size}")
    rtpt = RTPT(name_initials='JS', experiment_name='grm_grp_training', max_iterations=args.epochs)
    rtpt.start()
    train_model(args, args.principle, args.input_type, device,
                log_wandb=True, n=100, epochs=args.epochs, data_num=args.data_num)
